{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-processing. POS Tagging. Sentence parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "text = '''London is considered to be one of the world's most important global cities \n",
    "and has been termed the world's most powerful, most desirable, most influential, most visited, most expensive, \n",
    "innovative, sustainable, most investment friendly, most popular for work, and the most vegetarian friendly city in the world. \n",
    "London exerts a considerable impact upon the arts, commerce, education, entertainment, fashion, finance, healthcare, media, \n",
    "professional services, research and development, tourism and transportation. London ranks 26 out of 300 major cities for economic  \n",
    "performance.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Разбиение текста на предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "simple_sentences = nltk.tokenize.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Разбиение текста на слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences = [nltk.tokenize.word_tokenize(sentence) for sentence in simple_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemmer Porter (Snowball)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.SnowballStemmer(language=\"english\")\n",
    "print([stemmer.stem(word) for word in sentences[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizer WordNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nltk.download(\"wordnet\") - thesaurus, ontology\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "print([wnl.lemmatize(t) for t in sentences[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemmer Lancaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens = text.split(' ')\n",
    "lancaster = nltk.stem.LancasterStemmer() # nltk.LancasterStemmer()\n",
    "print([lancaster.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords, reuters\n",
    "\n",
    "def content_fraction(text):\n",
    "    stopwords_en = stopwords.words(\"english\")\n",
    "    content = [w for w in text if w.lower() not in stopwords_en]\n",
    "    return len(content) / len(text)\n",
    "\n",
    "# nltk.download(\"reuters\")\n",
    "print(content_fraction(reuters.words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "tagged_sentence = nltk.pos_tag(sentences[0])\n",
    "print(tagged_sentence)\n",
    "\n",
    "tree = nltk.ne_chunk(tagged_sentence)\n",
    "tree.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# TF-IDF\n",
    "def compute_tfidf(corpus):\n",
    "    def compute_tf(text):\n",
    "        tf_text = Counter(text)\n",
    "        for i in tf_text:\n",
    "            tf_text[i] = tf_text[i] / float(len(tf_text))\n",
    "        return tf_text\n",
    "\n",
    "    def compute_idf(word, corpus):\n",
    "        return math.log10(len(corpus) / sum([1.0 for i in corpus if word in i]))\n",
    "\n",
    "    documents_list = []\n",
    "    for text in corpus:\n",
    "        tf_idf_dictionary = {}\n",
    "        computed_tf = compute_tf(text)\n",
    "        for word in computed_tf:\n",
    "            tf_idf_dictionary[word] = computed_tf[word] * compute_idf(word, corpus)\n",
    "        documents_list.append(tf_idf_dictionary)    \n",
    "    return documents_list   \n",
    "\n",
    "corpus = [\n",
    "    ['pasta', 'la', 'vista', 'baby', 'la', 'vista'], \n",
    "    ['hasta', 'siempre', 'comandante', 'baby', 'la', 'siempre'], \n",
    "    ['siempre', 'comandante', 'baby', 'la', 'siempre']\n",
    "]    \n",
    "\n",
    "print(compute_tfidf(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "english_stemmer = EnglishStemmer()\n",
    "\n",
    "class StemmedCountVectorizer(TfidfVectorizer):\n",
    "    def build_anyalyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([english_stemmer.stem(w) for w in analyzer(doc)])\n",
    "\n",
    "stem_vectorizer = StemmedCountVectorizer(stop_words='english')   \n",
    "\n",
    "print(stem_vectorizer.get_stop_words())\n",
    "print(\"***\")\n",
    "print(stem_vectorizer.fit_transform(simple_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "sentence = \"Steve Key, Mark Norkin and John Spenser are working in the Westminster Christian Academy\"\n",
    "\n",
    "from nltk.tag import PerceptronTagger\n",
    "from nltk.data import find\n",
    "\n",
    "PICKLE = \"averaged_perceptron_tagger.pickle\"\n",
    "AP_MODEL_LOC = \"file:\" + str(find('taggers/averaged_perceptron_tagger/' + PICKLE))\n",
    "\n",
    "tagger = PerceptronTagger(load = False)\n",
    "tagger.load(AP_MODEL_LOC)\n",
    "pos_tag = tagger.tag\n",
    "\n",
    "print(ne_chunk(pos_tag(word_tokenize(sentence))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "IN = re.compile(r'.*\\bin\\b(?!\\b.+ing)')\n",
    "\n",
    "# nltk.download('ieer')\n",
    "for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n",
    "    for rel in nltk.sem.extract_rels('ORG', 'LOC', doc, corpus = 'ieer', pattern = IN):\n",
    "        print(nltk.sem.rtuple(rel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "\n",
    "def get_continuous_chunks(text):\n",
    "    chunked = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "    for subtree in chunked:\n",
    "        if type(subtree) == Tree:\n",
    "            current_chunk.append(\" \".join([token for token, pos in subtree.leaves()]))\n",
    "        elif current_chunk:\n",
    "            named_entity = \" \".join(current_chunk)  \n",
    "            if named_entity not in continuous_chunk:\n",
    "                continuous_chunk.append(named_entity)   # { named_entity: subtree[1] }\n",
    "                current_chunk = []\n",
    "        else:\n",
    "            continue\n",
    "    return continuous_chunk\n",
    "\n",
    "text = \"Mark Norkin and John Spenser are working in New York.\"\n",
    "\n",
    "print(get_continuous_chunks(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence = \"Mark Norkin and John Spenser are working in the Westminster Christian Academy\"\n",
    "\n",
    "for sent in nltk.sent_tokenize(sentence):\n",
    "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "        if hasattr(chunk, 'label'):\n",
    "            print(chunk.label(), ''.join(c[0] for c in chunk))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

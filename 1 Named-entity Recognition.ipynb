{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-processing. POS Tagging. Sentence parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "text = '''London is considered to be one of the world's most important global cities \n",
    "and has been termed the world's most powerful, most desirable, most influential, most visited, most expensive, \n",
    "innovative, sustainable, most investment friendly, most popular for work, and the most vegetarian friendly city in the world. \n",
    "London exerts a considerable impact upon the arts, commerce, education, entertainment, fashion, finance, healthcare, media, \n",
    "professional services, research and development, tourism and transportation. London ranks 26 out of 300 major cities for economic  \n",
    "performance.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Разбиение текста на предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "simple_sentences = nltk.tokenize.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Разбиение текста на слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences = [nltk.tokenize.word_tokenize(sentence) for sentence in simple_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemmer Porter (Snowball)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['london', 'is', 'consid', 'to', 'be', 'one', 'of', 'the', 'world', \"'s\", 'most', 'import', 'global', 'citi', 'and', 'has', 'been', 'term', 'the', 'world', \"'s\", 'most', 'power', ',', 'most', 'desir', ',', 'most', 'influenti', ',', 'most', 'visit', ',', 'most', 'expens', ',', 'innov', ',', 'sustain', ',', 'most', 'invest', 'friend', ',', 'most', 'popular', 'for', 'work', ',', 'and', 'the', 'most', 'vegetarian', 'friend', 'citi', 'in', 'the', 'world', '.']\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.stem.SnowballStemmer(language=\"english\")\n",
    "print([stemmer.stem(word) for word in sentences[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizer WordNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['London', 'is', 'considered', 'to', 'be', 'one', 'of', 'the', 'world', \"'s\", 'most', 'important', 'global', 'city', 'and', 'ha', 'been', 'termed', 'the', 'world', \"'s\", 'most', 'powerful', ',', 'most', 'desirable', ',', 'most', 'influential', ',', 'most', 'visited', ',', 'most', 'expensive', ',', 'innovative', ',', 'sustainable', ',', 'most', 'investment', 'friendly', ',', 'most', 'popular', 'for', 'work', ',', 'and', 'the', 'most', 'vegetarian', 'friendly', 'city', 'in', 'the', 'world', '.']\n"
     ]
    }
   ],
   "source": [
    "# nltk.download(\"wordnet\") - thesaurus, ontology\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "print([wnl.lemmatize(t) for t in sentences[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemmer Lancaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['london', 'is', 'consid', 'to', 'be', 'on', 'of', 'the', \"world's\", 'most', 'import', 'glob', 'city', '\\nand', 'has', 'been', 'term', 'the', \"world's\", 'most', 'powerful,', 'most', 'desirable,', 'most', 'influential,', 'most', 'visited,', 'most', 'expensive,', '\\ninnovative,', 'sustainable,', 'most', 'invest', 'friendly,', 'most', 'popul', 'for', 'work,', 'and', 'the', 'most', 'veget', 'friend', 'city', 'in', 'the', 'world.', '\\nlondon', 'exert', 'a', 'consid', 'impact', 'upon', 'the', 'arts,', 'commerce,', 'education,', 'entertainment,', 'fashion,', 'finance,', 'healthcare,', 'media,', '\\nprofessional', 'services,', 'research', 'and', 'development,', 'tour', 'and', 'transportation.', 'london', 'rank', '26', 'out', 'of', '300', 'maj', 'city', 'for', 'econom', '', '\\nperformance.']\n"
     ]
    }
   ],
   "source": [
    "tokens = text.split(' ')\n",
    "lancaster = nltk.stem.LancasterStemmer() # nltk.LancasterStemmer()\n",
    "print([lancaster.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.735240435097661\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords, reuters\n",
    "\n",
    "def content_fraction(text):\n",
    "    stopwords_en = stopwords.words(\"english\")\n",
    "    content = [w for w in text if w.lower() not in stopwords_en]\n",
    "    return len(content) / len(text)\n",
    "\n",
    "# nltk.download(\"reuters\")\n",
    "print(content_fraction(reuters.words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('London', 'NNP'), ('is', 'VBZ'), ('considered', 'VBN'), ('to', 'TO'), ('be', 'VB'), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('world', 'NN'), (\"'s\", 'VBZ'), ('most', 'RBS'), ('important', 'JJ'), ('global', 'JJ'), ('cities', 'NNS'), ('and', 'CC'), ('has', 'VBZ'), ('been', 'VBN'), ('termed', 'VBN'), ('the', 'DT'), ('world', 'NN'), (\"'s\", 'POS'), ('most', 'RBS'), ('powerful', 'JJ'), (',', ','), ('most', 'JJS'), ('desirable', 'JJ'), (',', ','), ('most', 'JJS'), ('influential', 'JJ'), (',', ','), ('most', 'JJS'), ('visited', 'VBN'), (',', ','), ('most', 'RBS'), ('expensive', 'JJ'), (',', ','), ('innovative', 'JJ'), (',', ','), ('sustainable', 'JJ'), (',', ','), ('most', 'JJS'), ('investment', 'NN'), ('friendly', 'RB'), (',', ','), ('most', 'JJS'), ('popular', 'JJ'), ('for', 'IN'), ('work', 'NN'), (',', ','), ('and', 'CC'), ('the', 'DT'), ('most', 'RBS'), ('vegetarian', 'JJ'), ('friendly', 'JJ'), ('city', 'NN'), ('in', 'IN'), ('the', 'DT'), ('world', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "tagged_sentence = nltk.pos_tag(sentences[0])\n",
    "print(tagged_sentence)\n",
    "\n",
    "tree = nltk.ne_chunk(tagged_sentence)\n",
    "tree.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'pasta': 0.11928031367991561, 'la': 0.0, 'vista': 0.23856062735983122, 'baby': 0.0}, {'hasta': 0.09542425094393249, 'siempre': 0.0704365036222725, 'comandante': 0.03521825181113625, 'baby': 0.0, 'la': 0.0}, {'siempre': 0.08804562952784062, 'comandante': 0.04402281476392031, 'baby': 0.0, 'la': 0.0}]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# TF-IDF\n",
    "def compute_tfidf(corpus):\n",
    "    def compute_tf(text):\n",
    "        tf_text = Counter(text)\n",
    "        for i in tf_text:\n",
    "            tf_text[i] = tf_text[i] / float(len(tf_text))\n",
    "        return tf_text\n",
    "\n",
    "    def compute_idf(word, corpus):\n",
    "        return math.log10(len(corpus) / sum([1.0 for i in corpus if word in i]))\n",
    "\n",
    "    documents_list = []\n",
    "    for text in corpus:\n",
    "        tf_idf_dictionary = {}\n",
    "        computed_tf = compute_tf(text)\n",
    "        for word in computed_tf:\n",
    "            tf_idf_dictionary[word] = computed_tf[word] * compute_idf(word, corpus)\n",
    "        documents_list.append(tf_idf_dictionary)    \n",
    "    return documents_list   \n",
    "\n",
    "corpus = [\n",
    "    ['pasta', 'la', 'vista', 'baby', 'la', 'vista'], \n",
    "    ['hasta', 'siempre', 'comandante', 'baby', 'la', 'siempre'], \n",
    "    ['siempre', 'comandante', 'baby', 'la', 'siempre']\n",
    "]    \n",
    "\n",
    "print(compute_tfidf(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'mill', 'when', 'others', 'whether', 'again', 'whereas', 'yourself', 'co', 'for', 'throughout', 'even', 'seems', 'toward', 'once', 'both', 'herself', 'if', 'call', 'well', 'and', 'been', 'rather', 'here', 'too', 'etc', 'will', 'after', 'except', 'must', 'now', 'anyhow', 'might', 'thru', 'yours', 'back', 'over', 'what', 'only', 'these', 'do', 'whatever', 'anything', 'keep', 'name', 'find', 'them', 'forty', 'yet', 'go', 'however', 'hereby', 'sometime', 'within', 'us', 'see', 'found', 'several', 'become', 'amount', 'as', 'our', 'whereby', 'thence', 'whither', 'cannot', 'everywhere', 'you', 'himself', 'further', 'mine', 'much', 'between', 'neither', 'top', 'alone', 'beside', 'onto', 'about', 'take', 'along', 'therein', 'ourselves', 'hasnt', 'that', 'six', 'she', 'then', 'out', 'together', 'almost', 'ours', 'never', 'hers', 'or', 'ten', 'whole', 'no', 'am', 'whenever', 'wherein', 'eg', 'off', 'describe', 'was', 'via', 'beforehand', 're', 'somewhere', 'give', 'least', 'noone', 'sometimes', 'whom', 'hereafter', 'own', 'whoever', 'would', 'while', 'amoungst', 'latter', 'afterwards', 'this', 'without', 'serious', 'anywhere', 'before', 'get', 'former', 'across', 'we', 'herein', 'nine', 'fifty', 'indeed', 'hundred', 'other', 'myself', 'someone', 'those', 'eight', 'seemed', 'else', 'in', 'my', 'nobody', 'the', 'which', 'where', 'thereupon', 'above', 'whose', 'have', 'nor', 'her', 'put', 'than', 'although', 'next', 'same', 'latterly', 'through', 'becomes', 'so', 'done', 'had', 'something', 'such', 'your', 'still', 'among', 'i', 'until', 'should', 'below', 'everything', 'hence', 'anyway', 'side', 'any', 'bill', 'de', 'fifteen', 'me', 'front', 'perhaps', 'few', 'part', 'thin', 'twelve', 'whence', 'con', 'him', 'more', 'by', 'thick', 'less', 'anyone', 'also', 'may', 'an', 'has', 'detail', 'fire', 'interest', 'itself', 'nevertheless', 'beyond', 'move', 'meanwhile', 'none', 'not', 'every', 'is', 'last', 'un', 'upon', 'since', 'were', 'who', 'from', 'nothing', 'due', 'sincere', 'therefore', 'third', 'empty', 'inc', 'during', 'per', 'ever', 'system', 'first', 'made', 'up', 'either', 'full', 'mostly', 'why', 'how', 'behind', 'all', 'but', 'besides', 'often', 'he', 'already', 'whereafter', 'one', 'there', 'being', 'be', 'eleven', 'ie', 'its', 'thereafter', 'yourselves', 'because', 'very', 'please', 'somehow', 'of', 'his', 'some', 'otherwise', 'their', 'they', 'three', 'towards', 'whereupon', 'fill', 'many', 'five', 'amongst', 'sixty', 'at', 'into', 'ltd', 'around', 'seem', 'to', 'twenty', 'wherever', 'moreover', 'hereupon', 'against', 'bottom', 'it', 'themselves', 'are', 'with', 'show', 'another', 'four', 'namely', 'formerly', 'couldnt', 'under', 'though', 'everyone', 'can', 'each', 'could', 'a', 'most', 'two', 'becoming', 'cant', 'cry', 'down', 'on', 'always', 'thus', 'seeming', 'elsewhere', 'thereby', 'nowhere', 'enough', 'became'})\n",
      "***\n",
      "  (0, 25)\t0.1079622807092414\n",
      "  (0, 7)\t0.18279603118967344\n",
      "  (0, 42)\t0.5483880935690203\n",
      "  (0, 21)\t0.18279603118967344\n",
      "  (0, 18)\t0.18279603118967344\n",
      "  (0, 3)\t0.13902109337029686\n",
      "  (0, 36)\t0.18279603118967344\n",
      "  (0, 30)\t0.18279603118967344\n",
      "  (0, 8)\t0.18279603118967344\n",
      "  (0, 22)\t0.18279603118967344\n",
      "  (0, 40)\t0.18279603118967344\n",
      "  (0, 14)\t0.18279603118967344\n",
      "  (0, 23)\t0.18279603118967344\n",
      "  (0, 35)\t0.18279603118967344\n",
      "  (0, 24)\t0.18279603118967344\n",
      "  (0, 17)\t0.3655920623793469\n",
      "  (0, 29)\t0.18279603118967344\n",
      "  (0, 41)\t0.18279603118967344\n",
      "  (0, 39)\t0.18279603118967344\n",
      "  (0, 4)\t0.18279603118967344\n",
      "  (1, 25)\t0.14179803913686784\n",
      "  (1, 13)\t0.24008495017351658\n",
      "  (1, 6)\t0.24008495017351658\n",
      "  (1, 20)\t0.24008495017351658\n",
      "  (1, 2)\t0.24008495017351658\n",
      "  (1, 5)\t0.24008495017351658\n",
      "  (1, 11)\t0.24008495017351658\n",
      "  (1, 12)\t0.24008495017351658\n",
      "  (1, 15)\t0.24008495017351658\n",
      "  (1, 16)\t0.24008495017351658\n",
      "  (1, 19)\t0.24008495017351658\n",
      "  (1, 27)\t0.24008495017351658\n",
      "  (1, 31)\t0.24008495017351658\n",
      "  (1, 34)\t0.24008495017351658\n",
      "  (1, 33)\t0.24008495017351658\n",
      "  (1, 9)\t0.24008495017351658\n",
      "  (1, 37)\t0.24008495017351658\n",
      "  (1, 38)\t0.24008495017351658\n",
      "  (2, 25)\t0.22440141104916914\n",
      "  (2, 3)\t0.28895767404089806\n",
      "  (2, 32)\t0.3799446164315741\n",
      "  (2, 0)\t0.3799446164315741\n",
      "  (2, 1)\t0.3799446164315741\n",
      "  (2, 26)\t0.3799446164315741\n",
      "  (2, 10)\t0.3799446164315741\n",
      "  (2, 28)\t0.3799446164315741\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "english_stemmer = EnglishStemmer()\n",
    "\n",
    "class StemmedCountVectorizer(TfidfVectorizer):\n",
    "    def build_anyalyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([english_stemmer.stem(w) for w in analyzer(doc)])\n",
    "\n",
    "stem_vectorizer = StemmedCountVectorizer(stop_words='english')   \n",
    "\n",
    "print(stem_vectorizer.get_stop_words())\n",
    "print(\"***\")\n",
    "print(stem_vectorizer.fit_transform(simple_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Steve/NNP)\n",
      "  (ORGANIZATION Key/NNP)\n",
      "  ,/,\n",
      "  (PERSON Mark/NNP Norkin/NNP)\n",
      "  and/CC\n",
      "  (PERSON John/NNP Spenser/NNP)\n",
      "  are/VBP\n",
      "  working/VBG\n",
      "  in/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION Westminster/NNP Christian/NNP Academy/NNP))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "sentence = \"Steve Key, Mark Norkin and John Spenser are working in the Westminster Christian Academy\"\n",
    "\n",
    "from nltk.tag import PerceptronTagger\n",
    "from nltk.data import find\n",
    "\n",
    "PICKLE = \"averaged_perceptron_tagger.pickle\"\n",
    "AP_MODEL_LOC = \"file:\" + str(find('taggers/averaged_perceptron_tagger/' + PICKLE))\n",
    "\n",
    "tagger = PerceptronTagger(load = False)\n",
    "tagger.load(AP_MODEL_LOC)\n",
    "pos_tag = tagger.tag\n",
    "\n",
    "print(ne_chunk(pos_tag(word_tokenize(sentence))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ORG: 'WHYY'] 'in' [LOC: 'Philadelphia']\n",
      "[ORG: 'McGlashan &AMP; Sarrail'] 'firm in' [LOC: 'San Mateo']\n",
      "[ORG: 'Freedom Forum'] 'in' [LOC: 'Arlington']\n",
      "[ORG: 'Brookings Institution'] ', the research group in' [LOC: 'Washington']\n",
      "[ORG: 'Idealab'] ', a self-described business incubator based in' [LOC: 'Los Angeles']\n",
      "[ORG: 'Open Text'] ', based in' [LOC: 'Waterloo']\n",
      "[ORG: 'WGBH'] 'in' [LOC: 'Boston']\n",
      "[ORG: 'Bastille Opera'] 'in' [LOC: 'Paris']\n",
      "[ORG: 'Omnicom'] 'in' [LOC: 'New York']\n",
      "[ORG: 'DDB Needham'] 'in' [LOC: 'New York']\n",
      "[ORG: 'Kaplan Thaler Group'] 'in' [LOC: 'New York']\n",
      "[ORG: 'BBDO South'] 'in' [LOC: 'Atlanta']\n",
      "[ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "IN = re.compile(r'.*\\bin\\b(?!\\b.+ing)')\n",
    "\n",
    "# nltk.download('ieer')\n",
    "for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n",
    "    for rel in nltk.sem.extract_rels('ORG', 'LOC', doc, corpus = 'ieer', pattern = IN):\n",
    "        print(nltk.sem.rtuple(rel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mark Norkin', 'John Spenser', 'New York']\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "\n",
    "def get_continuous_chunks(text):\n",
    "    chunked = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "    for subtree in chunked:\n",
    "        if type(subtree) == Tree:\n",
    "            current_chunk.append(\" \".join([token for token, pos in subtree.leaves()]))\n",
    "        elif current_chunk:\n",
    "            named_entity = \" \".join(current_chunk)  \n",
    "            if named_entity not in continuous_chunk:\n",
    "                continuous_chunk.append(named_entity)   # { named_entity: subtree[1] }\n",
    "                current_chunk = []\n",
    "        else:\n",
    "            continue\n",
    "    return continuous_chunk\n",
    "\n",
    "text = \"Mark Norkin and John Spenser are working in New York.\"\n",
    "\n",
    "print(get_continuous_chunks(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON Mark\n",
      "ORGANIZATION Norkin\n",
      "PERSON JohnSpenser\n",
      "ORGANIZATION WestminsterChristianAcademy\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Mark Norkin and John Spenser are working in the Westminster Christian Academy\"\n",
    "\n",
    "for sent in nltk.sent_tokenize(sentence):\n",
    "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "        if hasattr(chunk, 'label'):\n",
    "            print(chunk.label(), ''.join(c[0] for c in chunk))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
